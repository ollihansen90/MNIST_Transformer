{'inner_dim': 310, 'transformer_depth': 3, 'attn_heads': 16, 'dim_head': 124, 'mlp_dim': 128, 'transformer_dropout': 0.0, 'num_classes': 62}
class_token: 310
pos_emb: 5270
projector.weight: 15190
transfomer.layers.0.0.norm.weight: 310
transfomer.layers.0.0.norm.bias: 310
transfomer.layers.0.0.func.to_qkv.weight: 1845120
transfomer.layers.0.0.func.out.0.weight: 615040
transfomer.layers.0.0.func.out.0.bias: 310
transfomer.layers.0.1.norm.weight: 310
transfomer.layers.0.1.norm.bias: 310
transfomer.layers.0.1.func.linear.0.weight: 39680
transfomer.layers.0.1.func.linear.0.bias: 128
transfomer.layers.0.1.func.linear.3.weight: 39680
transfomer.layers.0.1.func.linear.3.bias: 310
transfomer.layers.1.0.norm.weight: 310
transfomer.layers.1.0.norm.bias: 310
transfomer.layers.1.0.func.to_qkv.weight: 1845120
transfomer.layers.1.0.func.out.0.weight: 615040
transfomer.layers.1.0.func.out.0.bias: 310
transfomer.layers.1.1.norm.weight: 310
transfomer.layers.1.1.norm.bias: 310
transfomer.layers.1.1.func.linear.0.weight: 39680
transfomer.layers.1.1.func.linear.0.bias: 128
transfomer.layers.1.1.func.linear.3.weight: 39680
transfomer.layers.1.1.func.linear.3.bias: 310
transfomer.layers.2.0.norm.weight: 310
transfomer.layers.2.0.norm.bias: 310
transfomer.layers.2.0.func.to_qkv.weight: 1845120
transfomer.layers.2.0.func.out.0.weight: 615040
transfomer.layers.2.0.func.out.0.bias: 310
transfomer.layers.2.1.norm.weight: 310
transfomer.layers.2.1.norm.bias: 310
transfomer.layers.2.1.func.linear.0.weight: 39680
transfomer.layers.2.1.func.linear.0.bias: 128
transfomer.layers.2.1.func.linear.3.weight: 39680
transfomer.layers.2.1.func.linear.3.bias: 310
outMLP.0.weight: 310
outMLP.0.bias: 310
outMLP.1.weight: 19220
outMLP.1.bias: 62
Total parameter count: 7665196